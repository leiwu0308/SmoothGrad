{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce the result of *SmoothGrad: removing noise by adding noise*\n",
    "\n",
    "Link to the paper: https://arxiv.org/pdf/1706.03825.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from torch.autograd import grad,Variable\n",
    "\n",
    "import torchvision.models as models\n",
    "#------------------------------\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os,random\n",
    "import attack as A\n",
    "from utils import *\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "USE_GPU=True\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)\n",
    "random.seed(123)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "infer = Infer(USE_GPU)\n",
    "attack = A.Attack(USE_GPU)\n",
    "\n",
    "def print_top_pred(p):\n",
    "    sorted_idx = np.argsort(p)[::-1][:5]\n",
    "    print('------------->')\n",
    "    for i,idx in enumerate(sorted_idx):\n",
    "        print('(%.0f,%d,%s)\\t'%(100*p[idx],idx,idx2class[idx]),end='')\n",
    "    print('')\n",
    "\n",
    "def load_model(model):\n",
    "    net = model(pretrained=True)\n",
    "    net = nn.Sequential(net,nn.Softmax())\n",
    "    if USE_GPU:\n",
    "        net = net.cuda()\n",
    "    return net\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    img = PIL2tensor(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /home/leiwu/.torch/models/resnet152-b121ed2d.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "alexnet = load_model(models.alexnet)\n",
    "vgg11   = load_model(models.vgg11_bn)\n",
    "vgg19   = load_model(models.vgg19_bn)\n",
    "resnet152  = load_model(models.resnet152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record\n",
    "\n",
    "  - For the same adversarial example generation method, different hyper-parameters, such as step size, number of step, will causes different transferability. For instance, (B=2,nstep=20) transfer, but not for (B=1,nstep=20), and (B=10,nstep=20)\n",
    "  - It seems that the adversarial examples generated by large step size transfer more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(\n",
    "    '/home/leiwu/lab/cleverhans/examples/nips17_adversarial_competition/dataset/images/014f0024918a7b6f.png')\n",
    "model  = Ensemble([vgg11,resnet50])\n",
    "model2 = resnet50\n",
    "\n",
    "ct = lambda x: 1*A.target(x,[345]) +1*A.negative_entropy(x)\n",
    "img_adv1 = attack(model,ct,img,epsilon=10,nstep=50,dt=20)\n",
    "p1 = infer(model,img)\n",
    "p2 = infer(model,img_adv1)\n",
    "p3 = infer(model2,img)\n",
    "p4 = infer(model2,img_adv1)\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,2,1); imshow_th(img,idx2class[p1.argmax()])\n",
    "plt.subplot(1,2,2); imshow_th(img_adv1,idx2class[p2.argmax()])\n",
    "\n",
    "print_top_pred(p1)\n",
    "print_top_pred(p2)\n",
    "print_top_pred(p3)\n",
    "print_top_pred(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = torch.rand(3,224,224) + img_adv1 - img\n",
    "img2 = torch.clamp(img2,0,1)\n",
    "p = infer(model,img2)\n",
    "print_top_pred(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(3,224,224).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# p3 = infer(model2,img)\n",
    "p4 = infer(model2,img_adv1)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(2,2,1); imshow_th(img,idx2class[p1.argmax()])\n",
    "plt.subplot(2,2,2); imshow_th(img_adv1,idx2class[p2.argmax()])\n",
    "plt.subplot(2,2,3);  imshow_th(img,idx2class[p3.argmax()])\n",
    "plt.subplot(2,2,4);  imshow_th(img_adv1,idx2class[p4.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argsort(p1)[::-1][::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2class[716]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the prediction changes along the epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = Image.open('./images/doberman.png')\n",
    "img = PIL2tensor(img)\n",
    "\n",
    "ct = confidence\n",
    "img_adv1 = FastGradAttack(model,ct,img,2)\n",
    "img_adv2 = FastGradAttack(model,ct,img,6)\n",
    "img_adv3 = FastGradAttack(model,ct,img,15)\n",
    "img_adv4 = FastGradAttack(model,ct,img,20)\n",
    "img_adv5 = FastGradAttack(model,ct,img,25)\n",
    "p0 = infer(model,img)\n",
    "p1 = infer(model,img_adv1)\n",
    "p2 = infer(model,img_adv2)\n",
    "p3 = infer(model,img_adv3)\n",
    "p4 = infer(model,img_adv4)\n",
    "p5 = infer(model,img_adv5)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(2,3,1); imshow(img,classes[p0.argmax()])\n",
    "plt.subplot(2,3,2); imshow(img_adv1,classes[p1.argmax()])\n",
    "plt.subplot(2,3,3); imshow(img_adv2,classes[p2.argmax()])\n",
    "plt.subplot(2,3,4); imshow(img_adv3,classes[p3.argmax()])\n",
    "plt.subplot(2,3,5); imshow(img_adv4,classes[p4.argmax()])\n",
    "plt.subplot(2,3,6); imshow(img_adv5,classes[p5.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "P=[torch.rand(3,4) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveat\n",
    "- Must use *eval model*, the forward propogation would change the batch statistics in *train model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image(model,img,smooth=True,nsamples=50,stddev=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = Image.open('./images/5.png')\n",
    "img = PIL2tensor(img)\n",
    "test_image(model,img,smooth=True,nsamples=10,stddev=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = Image.open('./images/16.png')\n",
    "img = PIL2tensor(img)\n",
    "test_image(model,img,smooth=True,nsamples=100,stddev=0.2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
